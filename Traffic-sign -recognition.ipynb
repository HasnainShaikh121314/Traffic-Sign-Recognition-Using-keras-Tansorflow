{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d4e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, BatchNormalization, Dropout, Dense,\n",
    "    Flatten, Input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=pd.read_csv(\"labels.csv\")\n",
    "classid=label['ClassId'].tolist()\n",
    "labelname=label['Name'].tolist()\n",
    "number_of_classes=len(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a8c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/DATA\\0\n",
      "dataset/DATA\\1\n",
      "dataset/DATA\\10\n",
      "dataset/DATA\\11\n",
      "dataset/DATA\\12\n",
      "dataset/DATA\\13\n",
      "dataset/DATA\\14\n",
      "dataset/DATA\\15\n",
      "dataset/DATA\\16\n",
      "dataset/DATA\\17\n",
      "dataset/DATA\\18\n",
      "dataset/DATA\\19\n",
      "dataset/DATA\\2\n",
      "dataset/DATA\\20\n",
      "dataset/DATA\\21\n",
      "dataset/DATA\\22\n",
      "dataset/DATA\\23\n",
      "dataset/DATA\\24\n",
      "dataset/DATA\\25\n",
      "dataset/DATA\\26\n",
      "dataset/DATA\\27\n",
      "dataset/DATA\\28\n",
      "dataset/DATA\\29\n",
      "dataset/DATA\\3\n",
      "dataset/DATA\\30\n",
      "dataset/DATA\\31\n",
      "dataset/DATA\\32\n",
      "dataset/DATA\\33\n",
      "dataset/DATA\\34\n",
      "dataset/DATA\\35\n",
      "dataset/DATA\\36\n",
      "dataset/DATA\\37\n",
      "dataset/DATA\\38\n",
      "dataset/DATA\\39\n",
      "dataset/DATA\\4\n",
      "dataset/DATA\\40\n",
      "dataset/DATA\\41\n",
      "dataset/DATA\\42\n",
      "dataset/DATA\\43\n",
      "dataset/DATA\\44\n",
      "dataset/DATA\\45\n",
      "dataset/DATA\\46\n",
      "dataset/DATA\\47\n",
      "dataset/DATA\\48\n",
      "dataset/DATA\\49\n",
      "dataset/DATA\\5\n",
      "dataset/DATA\\50\n",
      "dataset/DATA\\51\n",
      "dataset/DATA\\52\n",
      "dataset/DATA\\53\n",
      "dataset/DATA\\54\n",
      "dataset/DATA\\55\n",
      "dataset/DATA\\56\n",
      "dataset/DATA\\57\n",
      "dataset/DATA\\6\n",
      "dataset/DATA\\7\n",
      "dataset/DATA\\8\n",
      "dataset/DATA\\9\n",
      "Loaded images: (4170, 32, 32, 3)\n",
      "Labels: (4170,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "image_size = 32  # standard for traffic signs\n",
    "\n",
    "dataset_path = \"dataset/DATA\"\n",
    "class_names = os.listdir(dataset_path)  # e.g., ['Stop', 'SpeedLimit', 'TurnLeft']\n",
    "label_namesid= [int(i) for i in class_names]\n",
    "\n",
    "\n",
    "for label, class_name in zip(label_namesid,class_names):\n",
    "    class_folder = os.path.join(dataset_path, class_name)\n",
    "    print(class_folder)\n",
    "    for img_name in os.listdir(class_folder):\n",
    "        img_path = os.path.join(class_folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            continue  # skip unreadable images\n",
    "\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"Loaded images:\", X.shape)\n",
    "print(\"Labels:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X = X / 255.0  # Normalize pixel values\n",
    "y = to_categorical(y)  # One-hot encode class labels\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c82b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(number_of_classes, activation='softmax')  # dynamic output layer\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db2f3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56fa4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "53/53 [==============================] - 3s 51ms/step - loss: 0.3452 - accuracy: 0.8951 - val_loss: 0.2961 - val_accuracy: 0.9017\n",
      "Epoch 2/20\n",
      "53/53 [==============================] - 3s 52ms/step - loss: 0.3099 - accuracy: 0.9014 - val_loss: 0.2009 - val_accuracy: 0.9400\n",
      "Epoch 3/20\n",
      "53/53 [==============================] - 3s 53ms/step - loss: 0.2582 - accuracy: 0.9203 - val_loss: 0.1725 - val_accuracy: 0.9436\n",
      "Epoch 4/20\n",
      "53/53 [==============================] - 3s 51ms/step - loss: 0.2186 - accuracy: 0.9326 - val_loss: 0.1558 - val_accuracy: 0.9472\n",
      "Epoch 5/20\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.2033 - accuracy: 0.9379 - val_loss: 0.1522 - val_accuracy: 0.9580\n",
      "Epoch 6/20\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.1612 - accuracy: 0.9508 - val_loss: 0.0984 - val_accuracy: 0.9736\n",
      "Epoch 7/20\n",
      "53/53 [==============================] - 3s 52ms/step - loss: 0.1363 - accuracy: 0.9628 - val_loss: 0.0877 - val_accuracy: 0.9712\n",
      "Epoch 8/20\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.1217 - accuracy: 0.9607 - val_loss: 0.0821 - val_accuracy: 0.9808\n",
      "Epoch 9/20\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.1618 - accuracy: 0.9496 - val_loss: 0.0664 - val_accuracy: 0.9820\n",
      "Epoch 10/20\n",
      "53/53 [==============================] - 3s 53ms/step - loss: 0.1020 - accuracy: 0.9709 - val_loss: 0.0540 - val_accuracy: 0.9868\n",
      "Epoch 11/20\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0855 - accuracy: 0.9724 - val_loss: 0.0526 - val_accuracy: 0.9820\n",
      "Epoch 12/20\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.0730 - accuracy: 0.9796 - val_loss: 0.0592 - val_accuracy: 0.9892\n",
      "Epoch 13/20\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.0850 - accuracy: 0.9739 - val_loss: 0.0313 - val_accuracy: 0.9940\n",
      "Epoch 14/20\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.0508 - accuracy: 0.9862 - val_loss: 0.0328 - val_accuracy: 0.9892\n",
      "Epoch 15/20\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.0530 - accuracy: 0.9862 - val_loss: 0.0275 - val_accuracy: 0.9916\n",
      "Epoch 16/20\n",
      "53/53 [==============================] - 3s 53ms/step - loss: 0.0498 - accuracy: 0.9859 - val_loss: 0.0187 - val_accuracy: 0.9964\n",
      "Epoch 17/20\n",
      "53/53 [==============================] - 3s 52ms/step - loss: 0.0489 - accuracy: 0.9847 - val_loss: 0.0362 - val_accuracy: 0.9844\n",
      "Epoch 18/20\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 0.0489 - accuracy: 0.9874 - val_loss: 0.0192 - val_accuracy: 0.9928\n",
      "Epoch 19/20\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 0.0381 - accuracy: 0.9901 - val_loss: 0.0168 - val_accuracy: 0.9964\n",
      "Epoch 20/20\n",
      "53/53 [==============================] - 3s 52ms/step - loss: 0.0498 - accuracy: 0.9862 - val_loss: 0.0262 - val_accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cae2755de0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6083b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"traffic_sign_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d389f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(\"traffic_sign_model.h5\")\n",
    "\n",
    "# Load class names (order should match training)\n",
    "class_names = sorted(os.listdir(\"dataset\"))  # adjust path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce66e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = \"test1.png\"  # your image path here\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (32, 32))        # match training size\n",
    "img_normalized = img / 255.0           # normalize like training\n",
    "img_input = img_normalized.reshape(1, 32, 32, 3)  # reshape for model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44ebda57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Class: 0 (98.30%)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(img_input)\n",
    "predicted_class = np.argmax(prediction)\n",
    "confidence = np.max(prediction)\n",
    "\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class} ({confidence * 100:.2f}%)\")\n",
    "cv2.putText(img, f\"{label} ({confidence*100:.1f}%)\", (5, 25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "cv2.imshow(\"Prediction\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
